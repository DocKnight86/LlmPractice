# LlmPractice

Version 1

A local running LLM built into a Blazor App.

Requires .NET 9 and Docker running ollama llama3.1 on port 11434

Can easily switch to any other LLM - https://ollama.com/library

In Version 2 I plan to add support for deepseek-r1 and gemma3 all from the same interface.

All inspiration for this project came from: https://medium.com/scrum-and-coke/creating-a-web-api-with-net-9-to-interact-with-a-local-ollama-ai-instance-using-llama-3-1-41fcc3cceb8b

Quite a bit had already changed since the article was written, particularly with the Microsoft.Extensions.AI package, so it wasnâ€™t as simple as copying and pasting to get it working.

Watch a demo video here: 
(In version 2 there will be a "Thinking" indicator added. Feel free to jump around as some of the responses take some time due to the hardware limitations of my laptop.)

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/6Y4LnnlxGQk/0.jpg)](https://www.youtube.com/watch?v=6Y4LnnlxGQk)
