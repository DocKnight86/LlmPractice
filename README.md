# LlmPractice

A local running LLM built into a Blazor App.

Can easily switch to any other LLM - https://ollama.com/library

All inspiration for this project came from: https://medium.com/scrum-and-coke/creating-a-web-api-with-net-9-to-interact-with-a-local-ollama-ai-instance-using-llama-3-1-41fcc3cceb8b

Already quite a bit had changed from the writing of that article though regarding the Microsoft.Extensions.AI package.

Requires .NET 9, Docker running ollama llama3.1 on port 11434

Watch a demo video here: 
(In version 2 there will be a "Thinking" indicator added. Feel free to jump around as some of the responses take some time due to the hardware limitations of my laptop.)

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/6Y4LnnlxGQk/0.jpg)](https://www.youtube.com/watch?v=6Y4LnnlxGQk)
