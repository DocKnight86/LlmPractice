# LlmPractice

Version 2

Local running large language models built into a Blazor App.

Requires .NET 9 and Docker running the Ollama docker image with llama3.1:8b and llama2:7b installed on port 11434

A future update will add support for deepseek-r1, gemma3 from Google, and phi4 from Microsoft. All open source.

https://hub.docker.com/r/ollama/ollama - Official Ollama Docker image.

All inspiration for this project came from: https://medium.com/scrum-and-coke/creating-a-web-api-with-net-9-to-interact-with-a-local-ollama-ai-instance-using-llama-3-1-41fcc3cceb8b

Quite a bit had already changed since the article was written, particularly with the Microsoft.Extensions.AI package, so it wasnâ€™t as simple as copying and pasting to get it working but the article pointed me in the right direction and got me thinking!

**Watch the demo videos here:**

Version 2 Video

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/FcH_w3bsdZQ/0.jpg)](https://www.youtube.com/watch?v=FcH_w3bsdZQ)


Version 1 Video (depreciated)

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/6Y4LnnlxGQk/0.jpg)](https://www.youtube.com/watch?v=6Y4LnnlxGQk)
